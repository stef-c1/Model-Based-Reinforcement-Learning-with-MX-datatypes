1. environment_bfp.yml -> Conda environment file (This would be a very heavy one, as I just exported my current environment. You might not require all packages).
2. QPyLinear.py -> Torch linear layer definition with block-scaled GeMMs. Square and vector sizes are controlled by parameters "block_size_rows" and "block_size_cols". "block_quantizer_fine" function within this reshapes 1D / 2D / 3D inputs into a 3D tensor where the last 2 shapes are "block_size_rows" and "block_size_cols". Then performs a block based quantization (Rounds to nearest F32 representation) based on the outer axis (No. of exponents). Then reshapes back to original shape. It also automatically does zero padding as required based on the tile and input shapes.
3. reacher_runs_relu_random/logs/logs.mat -> Contains trajectory outputs obtained from MuJoCo. This would be the training data that the DNN needs to learn. Its basically state/action trajectories across time observed for a task that achieved high rewards. 
4. config_mx_mbrl/reacher_qpy2.py contains the model definition along with some other pre/post data processing functions.
5. block_scaled_training.ipynb -> Main training script and playground for other quantization explorations with distributions.